x-airflow-common:
    &airflow-common
    build:
        context: ./dockerfile_airflow
    environment:
        &airflow-common-env
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@pg-airflow/airflow
        AIRFLOW__CELERY__RESULT_BACKEND: redis://:@redis:6379/0
        AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
        AIRFLOW__CORE__FERNET_KEY: 5Dy0JFMRxClaIrfRjI1rQTj62igThKXvFnkd4SsVtiY=
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
        AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
        AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
        AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: "30"   # refresca DAGs cada 30s
        AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
        AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
        AIRFLOW__API_AUTH__JWT_SECRET: "b2b5f3b949859504d1272651e4cfaa9f6eac58fc5d55bc3e7b8b3fdb0ee89183"
        AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
        TZ: ${TZ}
    volumes:
        - ${WD}/airflow/dags:/opt/airflow/dags
        - ${WD}/airflow/logs:/opt/airflow/logs
        - ${WD}/airflow/plugins:/opt/airflow/plugins
        - ${WD}/airflow/include:/opt/airflow/include
        - ${ND}:/home/airflow/notebooks
        - /home/algotrading:/home/airflow/algotrading
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
        &airflow-common-depends-on
        redis:
            condition: service_healthy
        pg-airflow:
            condition: service_healthy

services:
    portainer:
        container_name: "portainer"
        image: portainer/portainer-ce:latest
        command: -H unix:///var/run/docker.sock
        restart: always
        ports:
            - 9000:9000
            - 8000:8000
        environment:
            TZ: ${TZ}
        #healthcheck:
        #    test: ["CMD", "curl", "--fail", "http://localhost:9000/api/status"]
        #    interval: 60s
        #    timeout: 5s
        #    retries: 3
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - portainer-volume:/data

    redis:
        image: redis
        container_name: "redis"
        volumes:
            - redis-volume:/data
        ports:
            - 6379:6379
        environment:
            TZ: ${TZ}
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 60s
            timeout: 30s
            retries: 10
        restart: always

    reverseproxy:
        container_name: "reverseproxy"
        build:
            context: ./dockerfile_reverseproxy
        ports:
            - 80:80
        healthcheck:
            test: ["CMD", "curl", "--silent", "--fail", "http://localhost:80/health"]
            interval: 60s
            timeout: 30s
            retries: 5
        environment:
            TZ: ${TZ}
        volumes:
            - ${ND}:/etc/nginx/html/
        restart: always

    pg-airflow:
        image: postgres
        container_name: "pg-airflow"
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
            TZ: ${TZ}
        ports:
            - 5432:5432
        volumes:
            - pg-airflow-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: unless-stopped
 
    pg-master:
        image: postgres:13
        container_name: "pg-master"
        ports:
            - 5431:5431            
        environment:
            SHARED_PASSWORD: password
            POSTGRES_PASSWORD: posgres349
            TZ: ${TZ}
        volumes:
            - ${WD}/postgress_db/scripts/:/docker-entrypoint-initdb.d/ 
            - pg-master-volume:/var/lib/postgresql/data
        healthcheck:
          test: ["CMD", "pg_isready", "-U", "postgres"]
          interval: 5s
          retries: 5
        restart: always

    pgadmin:
        image: dpage/pgadmin4
        container_name: "pg-admin"
        environment:
            PGADMIN_DEFAULT_EMAIL: "guest@guest.com"
            PGADMIN_DEFAULT_PASSWORD: "guest"
            TZ: ${TZ}
        volumes:
            - ${WD}/pgadmin/:/var/lib/pgadmin 
        ports:
            - 1234:80
        depends_on:
            - reverseproxy
            - pg-master
        
    airflow-apiserver:
        <<: *airflow-common
        container_name: "airflow-apiserver"
        command: api-server --apps all 
        ports:
            - 8080:8080
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/monitor/health"]
            interval: 30s
            timeout: 10s
            retries: 5
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
              condition: service_completed_successfully
       
    airflow-scheduler:
        <<: *airflow-common
        container_name: "airflow-scheduler"
        command: scheduler
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s

        restart: always
        depends_on:
           <<: *airflow-common-depends-on
           airflow-init:
              condition: service_completed_successfully

    airflow-dag-processor:
        <<: *airflow-common
        container_name: "airflow-dag-processor"
        command: dag-processor
        healthcheck:
          test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$$HOSTNAME"']
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always
        depends_on:
          <<: *airflow-common-depends-on
          airflow-init:
            condition: service_completed_successfully
     
    airflow-triggerer:
        <<: *airflow-common
        container_name: "airflow-triggerer"
        command: triggerer
        healthcheck:
          test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$$HOSTNAME"']
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always

    airflow-cli:
        <<: *airflow-common
        container_name: "airflow-cli"
        profiles:
          - debug
        environment:
          <<: *airflow-common-env
          CONNECTION_CHECK_MAX_COUNT: "0"
        command:
          - bash
          - -c
          - airflow
        depends_on:
          <<: *airflow-common-depends-on

    airflow-worker:
        <<: *airflow-common
        container_name: "airflow-worker"
        command: celery worker
        restart: always
        environment:
            <<: *airflow-common-env
        depends_on:
            <<: *airflow-common-depends-on

    airflow-flower:
        <<: *airflow-common
        container_name: "airflow-flower"
        command: celery flower
        ports:
            - 5555:5555
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
          interval: 30s
          timeout: 30s
          retries: 5
        restart: always

    airflow-init:
        <<: *airflow-common
        container_name: "airflow-init"
        entrypoint: /bin/bash
        command:
          - -c
          - |
            if [[ -z "${AIRFLOW_UID}" ]]; then
              echo
              echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
            fi
            mkdir -p /opt/airflow/{logs,dags,plugins,config}
            chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
            exec /entrypoint airflow version
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_MIGRATE: 'true'
            _AIRFLOW_WWW_USER_CREATE: 'true'
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
        user: "0:0"

    ib-gateway:
        container_name: "ib-gateway"
        build:
            context: ./ibgateway
        restart: always
        ports:
            - "7497:7497"
            - "4001:4001"
            - "5999:5999"
        environment:
            TZ: ${TZ}
            VNC_PASSWORD: ib
            VNC_PORT: 5999
            TWSUSERID: ${TWSUSERID}
            TWSPASSWORD: ${TWSPASSWORD}
            TRADING_MODE: ${TRADING_MODE}

    mt5:
        container_name: "mt5"
        build:
            context: ./dockerfile_mt5
        restart: always
        volumes:
          - ${WD}/config/:/config
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:3000"]
            interval: 30s
            timeout: 30s
            retries: 5
        ports:
          - "3000:3000"
          - "8001:8001"
        environment:
            UID: 1000
            GID: 1000
            CUSTOM_USER: ${CUSTOM_USER}
            PASSWORD: ${PASSWORD}

    metabase:
        container_name: "metabase"
        build:
            context: ./dockerfile_metabase
        restart: always
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:3000/api/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 60s
        environment:
           MB_DB_TYPE: postgres
           MB_DB_DBNAME: metabase
           MB_DB_PORT: 5432
           MB_DB_USER: metabase_user
           MB_DB_PASS: metabase_pass
           MB_DB_HOST: pg-master
        
           # Configuración opcional
           MB_ENCRYPTION_SECRET_KEY: "your-secret-key-here-change-this"
           JAVA_TIMEZONE: ${TZ}
           TZ: ${TZ}
        ports:
            - "3001:3000"
        depends_on:
            - pg-master
        volumes:
            - ${WD}/metabase/:/metabase-data 
            - ${ND}:/home/airflow/notebooks
    
volumes:
    redis-volume:
        external: false
    pg-master-volume:
        external: false
    pg-airflow-volume:
        external: false
    portainer-volume:
        external: false
    metabase-volume:  # <-- AÑADE ESTO
        external: false

networks:
  default:
      name: algotrading_stack_default

